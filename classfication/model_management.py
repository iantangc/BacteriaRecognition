"""Trains a ResNet on the CIFAR10 dataset.

ResNet v1
[a] Deep Residual Learning for Image Recognition
https://arxiv.org/pdf/1512.03385.pdf

ResNet v2
[b] Identity Mappings in Deep Residual Networks
https://arxiv.org/pdf/1603.05027.pdf
"""
from __future__ import print_function, division
import keras
from keras.layers import Dense, Add, Conv2D, BatchNormalization, Activation, GlobalAveragePooling2D
from keras.layers import MaxPooling2D, AveragePooling2D, ZeroPadding2D, Input, Flatten
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras.callbacks import ReduceLROnPlateau
from keras.preprocessing.image import ImageDataGenerator
from keras.regularizers import l2
from keras import backend as K
from keras.models import Model, load_model
from keras.utils import plot_model
from keras.utils.vis_utils import model_to_dot
from keras.datasets import cifar10
import numpy as np
import os
import sys
import h5py
from datetime import datetime

import pydot
import graphviz
# from IPython.display import SVG

from model_specification import *
from data_generator import *
import learning_parameters

# %matplotlib inline

def instantiate_new_model():
    model_parameters = learning_parameters.get_model_parameters()
    file_parameters = learning_parameters.get_file_parameters()
    training_parameters = learning_parameters.get_training_parameters()
    learning_rate_schedule = learning_parameters.learning_rate_schedule

    model, model_name = resnet_model(input_shape = model_parameters['input_shape'], num_output_classes = model_parameters['num_output_classes'], parameters = model_parameters['model_structure'])

    model.compile(loss = 'categorical_crossentropy',
                  optimizer = Adam(lr=learning_rate_schedule(0)),
                  metrics = ['accuracy'])
    print(model_name)
    print(model.summary())
    # plot_model(model, to_file='ResNetModel.png')
    # SVG(model_to_dot(model).create(prog='dot', format='svg'))

    if not os.path.isdir(file_parameters['save_dir']):
        os.makedirs(file_parameters['save_dir'])
    filepath = os.path.join(file_parameters['save_dir'], file_parameters['model_save_file_name'])
    
    model.save(filepath.format(epoch = 0))
    training_info_dictionary = {}
    training_info_dictionary['epoch'] = 0

    log_file_path = os.path.join(file_parameters['save_dir'], file_parameters['log_save_file_name'])
    np.save(log_file_path.format(epoch = 0), training_info_dictionary)

    print("Model saved at " + filepath.format(epoch = 0))
    print("Log file save at " + log_file_path.format(epoch = 0), training_info_dictionary)

def train_model(model_file_name, log_file_name):



    model_parameters = learning_parameters.get_model_parameters()
    file_parameters = learning_parameters.get_file_parameters()
    training_parameters = learning_parameters.get_training_parameters()
    learning_rate_schedule = learning_parameters.learning_rate_schedule

    if not os.path.isdir(file_parameters['save_dir']):
        os.makedirs(file_parameters['save_dir'])
    filepath = os.path.join(file_parameters['save_dir'], file_parameters['model_save_file_name'])


    if not os.path.isfile(model_file_name):
        print("A")
    else:
        print("B")

    log_dict = np.load(log_file_name).item()
    model = load_model(model_file_name)

    batch_size = training_parameters['batch_size']
    datagen = H5DataGenerator(file_parameters['data_file_path'], dim_x = model_parameters['input_shape'][0], 
        dim_y = model_parameters['input_shape'][1], dim_z = model_parameters['input_shape'][2], 
        batch_size = batch_size, shuffle = True, n_classes = model_parameters['num_output_classes'])

    # Prepare callbacks for model saving and for learning rate adjustment.
    checkpoint = ModelCheckpoint(filepath=filepath,
                                 monitor='val_acc',
                                 verbose=1,
                                 period=file_parameters['file_save_period'],
                                 save_best_only=True)

    lr_scheduler = LearningRateScheduler(learning_parameters.learning_rate_schedule)

    lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),
                                   cooldown=0,
                                   patience=5,
                                   min_lr=0.5e-6)

    callbacks = [checkpoint, lr_reducer, lr_scheduler]

    print('Training starts')
    tick = datetime.now()

    # Fit the model on the batches generated by datagen.flow().
    model.fit_generator(generator = datagen.generate("training"),
                        steps_per_epoch = int(np.ceil(datagen.get_dataset_size("training") / batch_size)),
                        validation_data = datagen.generate("dev"),
                        validation_steps =int(np.ceil(datagen.get_dataset_size("dev") / batch_size)),
                        epochs = training_parameters['epochs'] + log_dict['epoch'], 
                        verbose = 2, 
                        workers = 1,
                        initial_epoch = log_dict['epoch'],
                        callbacks = callbacks)

    tock = datetime.now()   
    diff_time = tock - tick
    print('Training Finished. Time taken:' + str(diff_time.total_seconds()))

    log_dict['epoch'] = log_dict['epoch'] + training_parameters['epochs']
    log_file_path = os.path.join(file_parameters['save_dir'], file_parameters['log_save_file_name'])
    np.save(log_file_path.format(epoch = log_dict['epoch']), log_dict)

    filepath = os.path.join(file_parameters['save_dir'], file_parameters['model_save_file_name']).format(epoch = log_dict['epoch'])
    if not os.path.isfile(filepath):
        model.save(filepath)

def evaluate_model(model_file_name):
    model_parameters = learning_parameters.get_model_parameters()
    file_parameters = learning_parameters.get_file_parameters()
    training_parameters = learning_parameters.get_training_parameters()

    model = load_model(model_file_name)

    datagen = H5DataGenerator(file_parameters['data_file_path'], dim_x = model_parameters['input_shape'][0], 
        dim_y = model_parameters['input_shape'][1], dim_z = model_parameters['input_shape'][2], 
        batch_size = batch_size, shuffle = True, n_classes = model_parameters['num_output_classes'])

    scores = model.evaluate_generator(generator = datagen.generate("testing"),
                            steps = int(np.ceil(datagen.get_dataset_size("testing") / batch_size)), 
                            workers = 1,
                            use_multiprocessing = False) 
    print('Test loss:', scores[0])
    print('Test accuracy:', scores[1])

if __name__ == "__main__":
    if len(sys.argv) > 1:
        if sys.argv[1] == 'init':
            instantiate_new_model()
        elif sys.argv[1] == 'train':
            if len(sys.argv) > 3:
                train_model(model_file_name = sys.argv[2], log_file_name = sys.argv[3])
            else:
                print("Usage: python *.py train model_file_path log_file_path")
        elif sys.argv[1] == 'test':
            if len(sys.argv) > 2:
                evaluate_model(model_file_name = sys.argv[2])
            else:
                print("Usage: python *.py test model_file_path")
    else:
        print("Usage: python *.py [init|train|test]")