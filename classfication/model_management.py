from __future__ import print_function, division
import keras
from keras.layers import Dense, Add, Conv2D, BatchNormalization, Activation, GlobalAveragePooling2D
from keras.layers import MaxPooling2D, AveragePooling2D, ZeroPadding2D, Input, Flatten
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras.callbacks import ReduceLROnPlateau
from keras.preprocessing.image import ImageDataGenerator
from keras.regularizers import l2
from keras import backend as K
from keras.models import Model, load_model
from keras.utils import plot_model
from keras.utils.vis_utils import model_to_dot
from keras.datasets import cifar10
import numpy as np
import os
import gc
import sys
import h5py
from datetime import datetime

# import pydot
# import graphviz
# from IPython.display import SVG

from model_specification import *
from data_interface import *
import learning_parameters

# %matplotlib inline

def instantiate_new_model(model_identifier = 'default'):
    # model identifier is a key within the model_structure dictionary
    model_parameters = learning_parameters.get_model_parameters()
    file_parameters = learning_parameters.get_file_parameters()
    training_parameters = learning_parameters.get_training_parameters()
    learning_rate_schedule = learning_parameters.learning_rate_schedule

    model, model_name = resnet_model(input_shape = model_parameters['input_shape'], num_output_classes = model_parameters['num_output_classes'], parameters = model_parameters['model_structure'][model_identifier])

    model.compile(loss = 'categorical_crossentropy',
                  optimizer = Adam(lr=learning_rate_schedule(0)),
                  metrics = ['accuracy'])
    print(model_name)
    print(model.summary())
    # plot_model(model, to_file='ResNetModel.png')
    # SVG(model_to_dot(model).create(prog='dot', format='svg'))

    if not os.path.isdir(file_parameters['save_dir']):
        os.makedirs(file_parameters['save_dir'])
    filepath = os.path.join(file_parameters['save_dir'], file_parameters['model_save_file_name'])
    
    model.save(filepath.format(epoch = 0))
    
    print("Model saved at " + filepath.format(epoch = 0))
    return filepath.format(epoch = 0)

def train_model(model_file_name, initial_epoch, num_epoch):

    model_parameters = learning_parameters.get_model_parameters()
    file_parameters = learning_parameters.get_file_parameters()
    training_parameters = learning_parameters.get_training_parameters()
    learning_rate_schedule = learning_parameters.learning_rate_schedule

    if not os.path.isdir(file_parameters['save_dir']):
        os.makedirs(file_parameters['save_dir'])
    filepath = os.path.join(file_parameters['save_dir'], file_parameters['model_save_file_name'])

    model = load_model(model_file_name)

    batch_size = training_parameters['batch_size']
    datagen = DataInterface(file_parameters['data_file_path'], dim_x = model_parameters['input_shape'][0], 
        dim_y = model_parameters['input_shape'][1], dim_z = model_parameters['input_shape'][2], 
        n_classes = model_parameters['num_output_classes'])

    # Prepare callbacks for model saving and for learning rate adjustment.
    checkpoint = ModelCheckpoint(filepath=filepath,
                                 monitor='val_acc',
                                 verbose=1,
                                 period=file_parameters['file_save_period'],
                                 save_best_only=False)

    lr_scheduler = LearningRateScheduler(learning_parameters.learning_rate_schedule)

    lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),
                                   cooldown=0,
                                   patience=5,
                                   min_lr=0.5e-6)

    callbacks = [checkpoint, lr_reducer, lr_scheduler]

    print('Training starts')
    tick = datetime.now()

    # Fit the model on the batches generated by datagen.flow().
    if training_parameters['generator_option'] == 'sequence':
        training_gen = datagen.get_sequence_generator("training", data_only = False, label_only = False, data_augmentation_enabled = training_parameters['data_augmentation'], shuffle = training_parameters['shuffle'], batch_size = batch_size, cached = training_parameters['cached_training'])
        validation_gen = datagen.get_sequence_generator("dev", data_only = False, label_only = False, data_augmentation_enabled = False, shuffle = False, batch_size = batch_size, cached = False)
        num_workers = training_parameters['sequence_generator_workers']
    else:
        training_gen = datagen.get_generator("training", data_only = False, label_only = False, data_augmentation_enabled = training_parameters['data_augmentation'], shuffle = training_parameters['shuffle'], batch_size = batch_size, cached = training_parameters['cached_training'])
        validation_gen = datagen.get_generator("dev", data_only = False, label_only = False, data_augmentation_enabled = False, shuffle = False, batch_size = batch_size, cached = False)
        num_workers = 1
        
    
    model.fit_generator(
        generator = training_gen,
        steps_per_epoch = int(np.ceil(datagen.get_dataset_size("training") / float(batch_size))),
        validation_data = validation_gen,
        validation_steps = int(np.ceil(datagen.get_dataset_size("dev") / float(batch_size))),
        epochs = num_epoch + initial_epoch, 
        verbose = training_parameters['training_verbose_option'], 
        workers = num_workers,
        initial_epoch = initial_epoch,
        callbacks = callbacks)

    tock = datetime.now()   
    diff_time = tock - tick
    print('Training Finished. Time taken:' + str(diff_time.total_seconds()))

    total_epoch = initial_epoch + num_epoch
    filepath = os.path.join(file_parameters['save_dir'], file_parameters['model_save_file_name']).format(epoch = total_epoch)
    if not os.path.isfile(filepath):
        print('Saving model')
        model.save(filepath)
        print('Model saved successfully')

def evaluate_model_overall(model_file_name):
    model_parameters = learning_parameters.get_model_parameters()
    file_parameters = learning_parameters.get_file_parameters()
    training_parameters = learning_parameters.get_training_parameters()

    model = load_model(model_file_name)
    
    batch_size = training_parameters['batch_size']

    datagen = DataInterface(file_parameters['data_file_path'], dim_x = model_parameters['input_shape'][0], 
        dim_y = model_parameters['input_shape'][1], dim_z = model_parameters['input_shape'][2], 
        n_classes = model_parameters['num_output_classes'])

    print('Evaluation starts')
    scores = model.evaluate_generator(generator = datagen.get_generator("testing", data_only = False, label_only = False, 
                                                              data_augmentation_enabled = False, shuffle = False, batch_size = batch_size, cached = False),
                            steps = int(np.ceil(datagen.get_dataset_size("testing") / batch_size)), 
                            workers = 1,
                            use_multiprocessing = False) 
    print('Test loss:', scores[0])
    print('Test accuracy:', scores[1])

if __name__ == "__main__":
    if len(sys.argv) > 1:
        if sys.argv[1] == 'init':
            if len(sys.argv) > 2:
                instantiate_new_model(sys.argv[2])
            else:
                print("Usage: python *.py init model_identifier")
        elif sys.argv[1] == 'train':
            if len(sys.argv) > 4:
                train_model(model_file_name = sys.argv[2], initial_epoch = int(sys.argv[3]), num_epoch = int(sys.argv[4]))
            else:
                print("Usage: python *.py train model_file_path initial_epoch num_epoch")
        elif sys.argv[1] == 'test':
            if len(sys.argv) > 2:
                evaluate_model_overall(model_file_name = sys.argv[2])
            else:
                print("Usage: python *.py test model_file_path")
    else:
        print("Usage: python *.py [init|train|test]")